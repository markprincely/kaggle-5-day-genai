{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84902fef",
   "metadata": {},
   "source": [
    "# Kaggle 5-Day Gen AI Capstone\n",
    "\n",
    "The objective is to use the Gemini API to demonstrate at least three (3) Gen AI capabilities. I plan to do most of them.\n",
    "\n",
    "1. [x] Structured output/JSON mode/controlled generation\n",
    "1. [x] Few-shot prompting\n",
    "1. [x] Understanding a document\n",
    "   - Text\n",
    "   - Image\n",
    "   - ~~Video~~\n",
    "   - ~~Audio~~\n",
    "1. [x] Function Calling\n",
    "1. [x] Agents\n",
    "1. ~~Long context window~~  Can process large documents\n",
    "1. ~~Context caching~~  Cache content for repetitive use, e.g. video analyses\n",
    "1. [ ] Gen AI evaluation\n",
    "1. ~~Grounding~~  Search results complement responses to maintain freshness.\n",
    "1. ~~Training~~  (Did similar at Aggregate Knowledge before BSchool)\n",
    "   - Embeddings  Embed data into a matrix, reduce dimensions to look it up.   \n",
    "   - Retrieval augmented generation (RAG)\n",
    "   - Vector search/vector store/vector database\n",
    "1. ~~MLOps (with GenAI)~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88a2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec592a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc42f31",
   "metadata": {},
   "source": [
    "### Pull your API key\n",
    "\n",
    "This notebook was written using Anaconda, rather than Kaggle secrets. Kaggle secrets code is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a2b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# \n",
    "# GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "# \n",
    "# client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"default_value\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5205a8",
   "metadata": {},
   "source": [
    "### Automated retry\n",
    "\n",
    "The this project is deep and sends a lot of requests, so set up an automatic retry\n",
    "that ensures your requests are retried when per-minute quota is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7e498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1343e39",
   "metadata": {},
   "source": [
    "## 1. Understanding text and image documents\n",
    "\n",
    "This section uses numerous models to evaluate a document. Source document was downloaded manually because it was prevented by the source server.\n",
    "\n",
    "Capstone objectives covered:\n",
    "* Understanding a text file\n",
    "* Understanding an image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adf413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -nv -O cpi.pdf https://www.bls.gov/news.release/pdf/cpi.pdf\n",
    "\n",
    "document_file = client.files.upload(file='cpi.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35769c8",
   "metadata": {},
   "source": [
    "### Summarizing a text document\n",
    "\n",
    "The summarisation request used here is fairly basic. It targets the training content specifically but provides no guidance otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465f3548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a summary of the Consumer Price Index (CPI) for July 2025, based on the provided news release:\n",
       "\n",
       "*   **Overall CPI:** The CPI for All Urban Consumers (CPI-U) increased by 0.2 percent in July on a seasonally adjusted basis, following a 0.3 percent increase in June. Over the last 12 months, the all items index increased 2.7 percent before seasonal adjustment.\n",
       "*   **Shelter:** The index for shelter rose 0.2 percent in July and was the primary factor in the all items monthly increase.\n",
       "*   **Food:** The food index was unchanged over the month as the food away from home index rose 0.3 percent while the food at home index fell 0.1 percent.\n",
       "*   **Energy:** The index for energy fell 1.1 percent in July as the index for gasoline decreased 2.2 percent over the month.\n",
       "*   **All Items Less Food and Energy:** The index for all items less food and energy rose 0.3 percent in July, following a 0.2-percent increase in June.\n",
       "*   **Other Notable Changes:** Indexes that increased over the month include medical care, airline fares, recreation, household furnishings and operations, and used cars and trucks. The indexes for lodging away from home and communication were among the few major indexes that decreased in July.\n",
       "*   **12-Month Changes:** The all items index rose 2.7 percent for the 12 months ending July. The all items less food and energy index rose 3.1 percent over the last 12 months. The energy index decreased 1.6 percent for the 12 months ending July. The food index increased 2.9 percent over the last year.\n",
       "*   **Wireless Telephone Services:** BLS has replaced survey data collected for the CPI's wireless telephone services index with secondary source data and non-traditional index methods."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = 'Tell me about the Consumer Price Index in July.'\n",
    "\n",
    "def summarize_doc(request: str) -> str:\n",
    "  \"\"\"Execute the request on the uploaded document.\"\"\"\n",
    "  # Set the temperature low to stabilise the output.\n",
    "  config = types.GenerateContentConfig(temperature=0.0)\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=config,\n",
    "      contents=[request, document_file],\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "summary = summarize_doc(request)\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95748d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's information about how the data is seasonally adjusted, based on the provided document:\n",
       "\n",
       "**How Seasonal Adjustment is Done**\n",
       "\n",
       "*   **Method:** The Consumer Price Index (CPI) program uses the X-13ARIMA-SEATS seasonal adjustment method to compute seasonally adjusted data.\n",
       "*   **Seasonal Factors:** Seasonal factors are derived using this method.\n",
       "*   **Annual Updates:** These factors are updated each February. The new factors are used to revise the previous 5 years of seasonally adjusted data.\n",
       "*   **Availability of Factors:** The seasonal factors are available at www.bls.gov/cpi/tables/seasonal-adjustment/seasonal-factors-2025.xlsx.\n",
       "*   **Intervention Analysis:** For some CPI series, the Bureau of Labor Statistics (BLS) uses intervention analysis seasonal adjustment (IASA). This process estimates and removes distortions caused by unusual events before calculating seasonal factors.\n",
       "\n",
       "**Why Seasonal Adjustment is Used**\n",
       "\n",
       "*   **Analyzing Short-Term Trends:** Seasonally adjusted changes are preferred for analyzing short-term price trends in the economy.\n",
       "*   **Eliminating Typical Changes:** Seasonal adjustment eliminates the effect of changes that normally occur at the same time and in about the same magnitude every year (e.g., weather events, production cycles, model changeovers, holidays, and sales).\n",
       "*   **Focusing on Atypical Changes:** This allows data users to focus on changes that are not typical for the time of year.\n",
       "\n",
       "**Important Considerations**\n",
       "\n",
       "*   **Unadjusted Data:** The unadjusted data are of primary interest to consumers concerned about the prices they actually pay.\n",
       "*   **Escalation Agreements:** BLS advises against using seasonally adjusted data in escalation agreements because seasonally adjusted series are revised annually for five years.\n",
       "*   **Revision of Indexes:** Seasonally adjusted data, including the U.S. city average all items index levels, are subject to revision for up to 5 years after their original release.\n",
       "*   **Determining Seasonal Status:** Each year, the seasonal status of every series is reevaluated based upon certain statistical criteria.\n",
       "\n",
       "I hope this is helpful!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request2 = 'Tell me about how the data is seasonally adjusted.'\n",
    "\n",
    "def summarize_doc(request: str) -> str:\n",
    "  \"\"\"Execute the request on the uploaded document.\"\"\"\n",
    "  # Set the temperature low to stabilise the output.\n",
    "  config = types.GenerateContentConfig(temperature=0.2)\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=config,\n",
    "      contents=[request, document_file],\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "summary2 = summarize_doc(request2)\n",
    "Markdown(summary2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d208a",
   "metadata": {},
   "source": [
    "### Define and employ an evaluation rubric\n",
    "\n",
    "Instructed the LLM to evaluate the response using a clear definition and [assessment rubric](https://en.wikipedia.org/wiki/Rubric_%28academic%29).\n",
    "\n",
    "#### Rubric\n",
    "1. How well the model followed the prompt (\"instruction following\")\n",
    "1. Whether it included relevant data in the prompt (\"groundedness\")\n",
    "1. How easy the text is to read (\"fluency\"), or other factors like \"verbosity\" or \"quality\".\n",
    "\n",
    "#### Prompt\n",
    "- Defines an evaluation agent using a pre-written \"summarization\" prompt\n",
    "- Uses it to gauge the quality of the previously-generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc03776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "STEP 1: The AI-generated response followed the instruction of the prompt. The response is grounded in the document provided. The response is well-organized and easy to read. The response did a good job extracting all the key information from the document provided.\n",
       "STEP 2: I would rate this response as very good.\n",
       "RATING: 5\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "# Define the evaluation prompt\n",
    "SUMMARY_PROMPT = \"\"\"\\\n",
    "# Instruction\n",
    "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n",
    "We will provide you with the user input and an AI-generated responses.\n",
    "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
    "You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "You will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n",
    "\n",
    "## Criteria\n",
    "Instruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.\n",
    "Groundedness: The response contains information included only in the context. The response does not reference any outside information.\n",
    "Conciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.\n",
    "Fluency: The response is well-organized and easy to read.\n",
    "\n",
    "## Rating Rubric\n",
    "5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.\n",
    "4: (Good). The summary follows instructions, is grounded, concise, and fluent.\n",
    "3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.\n",
    "2: (Bad). The summary is grounded, but does not follow the instructions.\n",
    "1: (Very bad). The summary is not grounded.\n",
    "\n",
    "## Evaluation Steps\n",
    "STEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\n",
    "STEP 2: Score based on the rubric.\n",
    "\n",
    "# User Inputs and AI-generated Response\n",
    "## User Inputs\n",
    "\n",
    "### Prompt\n",
    "{prompt}\n",
    "\n",
    "## AI-generated Response\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "# Define a structured enum class to capture the result.\n",
    "class SummaryRating(enum.Enum):\n",
    "  VERY_GOOD = '5'\n",
    "  GOOD = '4'\n",
    "  OK = '3'\n",
    "  BAD = '2'\n",
    "  VERY_BAD = '1'\n",
    "\n",
    "\n",
    "def eval_summary(prompt, ai_response):\n",
    "  \"\"\"Evaluate the generated summary against the prompt used.\"\"\"\n",
    "\n",
    "  chat = client.chats.create(model='gemini-2.0-flash')\n",
    "\n",
    "  # Generate the full text response.\n",
    "  response = chat.send_message(\n",
    "      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n",
    "  )\n",
    "  verbose_eval = response.text\n",
    "\n",
    "  # Coerce into the desired structure.\n",
    "  structured_output_config = types.GenerateContentConfig(\n",
    "      response_mime_type=\"text/x.enum\",\n",
    "      response_schema=SummaryRating,\n",
    "  )\n",
    "  response = chat.send_message(\n",
    "      message=\"Convert the final score.\",\n",
    "      config=structured_output_config,\n",
    "  )\n",
    "  structured_eval = response.parsed\n",
    "\n",
    "  return verbose_eval, structured_eval\n",
    "\n",
    "\n",
    "text_eval, struct_eval = eval_summary(prompt=[request, document_file], ai_response=summary)\n",
    "Markdown(text_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e1c0f",
   "metadata": {},
   "source": [
    "In this example, the model generated a textual justification that was set up in a chat context. This full text response is useful both for human interpretation and for giving the model a place to \"collect notes\" while it assesses the text and produces a final score. This \"note taking\" or \"thinking\" strategy typically works well with auto-regressive models, where the generated text is passed back into the model at each generation step. This means the working \"notes\" are used when generating final result output.\n",
    "\n",
    "In the next turn, the model converts the text output into a structured response. If you want to aggregate scores or use them programatically then you want to avoid parsing the unstructured text output. Here the `SummaryRating` schema is passed, so the model converts the chat history into an instance of the `SummaryRating` enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174d7795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SummaryRating.VERY_GOOD: '5'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad35a96",
   "metadata": {},
   "source": [
    "### Refine the summary prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4d0b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, imagine you have a big basket filled with all the things your family buys in a month: food, toys, clothes, gas for the car, and even going to the movies.\n",
       "\n",
       "The Consumer Price Index (CPI) is like a number that tells us if the price of that whole basket went up or down compared to last month.\n",
       "\n",
       "In July, the CPI went up by 0.2%. That means, on average, the things in your basket cost a little bit more than they did in June.\n",
       "\n",
       "Some things in the basket went up more than others. For example, the price of shelter (like renting an apartment) went up. But other things, like energy (like gasoline), went down.\n",
       "\n",
       "So, the CPI is just a way to see if things are getting more expensive or cheaper overall. In July, things got a little more expensive.\n",
       "-----"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation\n",
       "STEP 1: The AI response successfully simplifies and explains the Consumer Price Index (CPI) in a way that a 5-year-old could understand. It uses the analogy of a \"basket\" of goods to represent the CPI and explains how the index reflects whether the cost of these goods has increased or decreased compared to the previous month. It follows instructions, is grounded, is concise, and is fluent.\n",
       "\n",
       "STEP 2: Based on the rubric, the response is very good.\n",
       "\n",
       "## Rating\n",
       "5\n",
       "\n",
       "-----"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummaryRating.VERY_GOOD\n"
     ]
    }
   ],
   "source": [
    "new_prompt = \"Explain like I'm 5 the Consumer Price Index in July.\"\n",
    "\n",
    "def run_and_eval_summary(a_prompt):\n",
    "  \"\"\"Generate and evaluate the summary using the new prompt.\"\"\"\n",
    "  summary = summarize_doc(a_prompt)\n",
    "  display(Markdown(summary + '\\n-----'))\n",
    "\n",
    "  text, struct = eval_summary([a_prompt, document_file], summary)\n",
    "  display(Markdown(text + '\\n-----'))\n",
    "  print(struct)\n",
    "\n",
    "run_and_eval_summary(new_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560513b",
   "metadata": {},
   "source": [
    "### Understanding an image\n",
    "\n",
    "Take an image and test AI comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8c67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a medium-shot, slightly low-angle view of a section of a roof and the top of a utility pole against a backdrop of trees and a clear blue sky. \n",
       "\n",
       "\n",
       "The roof is covered in light brown composite shingles, with visible horizontal lines indicating the layered structure. Two vents are present on the roof. One is a silver, turbine-style vent with curved blades, likely for ventilation. The other is a shorter, gray cylindrical vent, possibly for plumbing or other utility exhaust. \n",
       "\n",
       "\n",
       "A wooden utility pole stands partially visible behind the roof. Several wires are strung from the pole, crossing the image horizontally. The wires are attached to the pole with gray insulators.\n",
       "\n",
       "\n",
       "Behind the pole and wires are the branches and leaves of several trees, likely eucalyptus, given their appearance and the visible peeling bark. The trees appear to be quite tall and full. The sky above is a clear, bright blue.\n",
       "-----"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Following the Evaluation Steps:\n",
       "\n",
       "**STEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and fluency according to the criteria.**\n",
       "\n",
       "* **Instruction following:** The response follows the instruction to describe the image. It provides a detailed description of the various elements present in the image.\n",
       "* **Groundedness:** The response appears to be grounded in the image provided, describing only what is visible.  I cannot confirm 100% since I don't have access to the image. However, the description does not hallucinate any details.\n",
       "* **Conciseness:** The response is slightly verbose. While it provides good detail, it could be more concise by combining some sentences. For example, the two vent descriptions could be combined into a single sentence.\n",
       "* **Fluency:** The response is well-organized and easy to read. The language used is clear and descriptive, and the structure flows logically from one element of the image to the next.\n",
       "\n",
       "**STEP 2: Score based on the rubric.**\n",
       "\n",
       "The response scores a **4 (Good)**. It follows instructions, is grounded, and fluent. While it provides a comprehensive description, it could be slightly more concise.\n",
       "\n",
       "-----"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummaryRating.GOOD\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "# Load the image\n",
    "img_filename = 'IMG_1125.jpg'\n",
    "img = PIL.Image.open(img_filename)\n",
    "\n",
    "# Assign the prompt\n",
    "img_prompt = \"Describe this image:\"\n",
    "# img_prompt = \"What type of tree is in the background of this image:\"\n",
    "\n",
    "def summarize_img(request: str) -> str:\n",
    "  \"\"\"Execute the request on the uploaded document.\"\"\"\n",
    "  # Set the temperature low to stabilise the output.\n",
    "  config = types.GenerateContentConfig(temperature=0.2)\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-1.5-pro-latest',\n",
    "      config=config,\n",
    "      contents=[request, img],\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "def eval_img_summary(prompt, ai_response):\n",
    "  \"\"\"Evaluate the generated image summary against the prompt used.\"\"\"\n",
    "\n",
    "  chat = client.chats.create(model='gemini-1.5-pro-latest')\n",
    "\n",
    "  # Generate the full text response.\n",
    "  response = chat.send_message(\n",
    "      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n",
    "  )\n",
    "  verbose_eval = response.text\n",
    "\n",
    "  # Coerce into the desired structure.\n",
    "  structured_output_config = types.GenerateContentConfig(\n",
    "      response_mime_type=\"text/x.enum\",\n",
    "      response_schema=SummaryRating,\n",
    "  )\n",
    "  response = chat.send_message(\n",
    "      message=\"Convert the final score.\",\n",
    "      config=structured_output_config,\n",
    "  )\n",
    "  structured_eval = response.parsed\n",
    "\n",
    "  return verbose_eval, structured_eval\n",
    "\n",
    "\n",
    "def run_and_eval_img_summary(prompt: str):\n",
    "  \"\"\"Generate and evaluate the image summary using the new prompt.\"\"\"\n",
    "  summary = summarize_img(prompt)\n",
    "  display(Markdown(summary + '\\n-----'))\n",
    "\n",
    "  text, struct = eval_img_summary(\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    -\n",
    "    prompt, img], summary)\n",
    "  display(Markdown(text + '\\n-----'))\n",
    "  print(struct)\n",
    "\n",
    "run_and_eval_img_summary(img_prompt)\n",
    "# summarize_img(img_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140180c9",
   "metadata": {},
   "source": [
    "## 2. Few-shot Prompting\n",
    "\n",
    "Let's modify the image understanding from a text prompt to a Few-Shot Prompt outputting JSON.\n",
    "\n",
    "Capstone objectives covered:\n",
    "* Structured prompting\n",
    "* Few-shot prompting \n",
    "* Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9331d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"size\": \"1280Wx960H\",\n",
      "\"mime-type\": \"image/jpeg\",\n",
      "\"contents\": \n",
      "[\n",
      "{   \n",
      "\"object\": \"turbine vent\", \n",
      "\"type\": \"roof turbine vent\", \n",
      "\"colors\": \"silver\",\n",
      "\"description\": \"a silver metal roof turbine vent\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"vent pipe\", \n",
      "\"type\": \"roof vent pipe\", \n",
      "\"colors\": \"silver\",\n",
      "\"description\": \"a silver metal roof vent pipe\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"roof\", \n",
      "\"type\": \"shingle roof\", \n",
      "\"colors\": \"brown\",\n",
      "\"description\": \"a brown shingle roof\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"tree\", \n",
      "\"type\": \"eucalyptus tree\", \n",
      "\"colors\": \"green, brown\",\n",
      "\"description\": \"a large eucalyptus tree with green leaves and brown branches\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"pole\", \n",
      "\"type\": \"utility pole\", \n",
      "\"colors\": \"gray\",\n",
      "\"description\": \"a gray wooden utility pole with wires\" \n",
      "}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Assign a few-shot prompt with examples.\n",
    "\n",
    "base_prompt = \"Using the following image, identify the image dimensions and mime type from file contents and catalog every object creating valid JSON:\"\n",
    "\n",
    "few_shot_examples = \"\"\"Using the following image, identify the image dimensions and mime type from file contents and catalog every object creating valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "For an 20 x 40 pixel (width x height) jpg image that has a basketball, a hand strainer, and a jade plant.\n",
    "\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"20Wx40H\",\n",
    "\"mime-type\": \"image/jpeg\",\n",
    "\"contents\": \n",
    "[\n",
    "{   \n",
    "\"object\": \"ball\", \n",
    "\"type\": \"basketball\", \n",
    "\"colors\": \"orange, black\",\n",
    "\"description\": \"an orange rubber basketball with faded black lines\" \n",
    "},\n",
    "{   \n",
    "\"object\": \"strainer\", \n",
    "\"type\": \"kitchen handheld strainer\", \n",
    "\"colors\": \"silver, blue\",\n",
    "\"description\": \"a small hand strainer for washing small fruit and vegetables\" \n",
    "},\n",
    "{   \n",
    "\"object\": \"plant\", \n",
    "\"type\": \"jade plant\", \n",
    "\"colors\": \"green, orange\",\n",
    "\"description\": \"a large, healthy jade plant with medium size leaves in an orange plastic pot\" \n",
    "}\n",
    "]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "For an 880 x 440 pixel (width x height) gif image that has a dog with a bone, a swimming pool, and a table with an parasol and a margarita.\n",
    "\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"880Wx440H\",\n",
    "\"mime-type\": \"image/gif\",\n",
    "\"contents\": \n",
    "[\n",
    "{   \n",
    "\"object\": \"dog\", \n",
    "\"type\": \"labradoodle\", \n",
    "\"colors\": \"white, black\",\n",
    "\"description\": \"a small black and white labradoodle lying on the ground chewing a bone\" \n",
    "},\n",
    "{   \n",
    "\"object\": \"bone\", \n",
    "\"type\": \"dog bone\", \n",
    "\"colors\": \"yellow\",\n",
    "\"description\": \"a small yellow dog bone chew toy\" \n",
    "},\n",
    "{   \n",
    "\"object\": \"pool\", \n",
    "\"type\": \"swimming pool\", \n",
    "\"colors\": \"brown, blue, white\",\n",
    "\"description\": \"a large brown round above ground swimming pool with white inner lining and blue water\" \n",
    "},\n",
    "{\n",
    "\"object\": \"table\", \n",
    "\"type\": \"patio table\", \n",
    "\"colors\": \"green, clear\",\n",
    "\"description\": \"a round green patio table with clear glass top that has a large parasol resting in a hole in the middle and a full margarita tumbler\" \n",
    "},\n",
    "{\n",
    "\"object\": \"umbrella\", \n",
    "\"type\": \"parasol\", \n",
    "\"colors\": \"green\",\n",
    "\"description\": \"a large open parasol creating shade behind the table\" \n",
    "},\n",
    "{\n",
    "\"object\": \"drink\", \n",
    "\"type\": \"margarita\", \n",
    "\"colors\": \"blue, yellow\",\n",
    "\"description\": \"a large blue goblet containing a yellow liquid and ice\" \n",
    "}\n",
    "]\n",
    "}\n",
    "``````\n",
    "\n",
    "Object Catalog:\n",
    "\"\"\"\n",
    "\n",
    "img_few_shot_prompt = base_prompt + few_shot_examples\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        response_mime_type=\"application/json\"\n",
    "    ),\n",
    "    contents=[img_few_shot_prompt, img])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d16af9",
   "metadata": {},
   "source": [
    "### Use extensions to fix hallucinations \n",
    "\n",
    "The JSON produced above is generally good, however the model can't figure out a way to extract metadata from the file without help.\n",
    "\n",
    "So, let's give it the help! \n",
    "* Dimensions: The PIL library extracts image dimensions as *img.size*.\n",
    "* MIME Type: The library mimetypes can take a swag. We can default it to \"image/*file-extension*\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b962c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - FUNCTION CALL: get_img_size(IMG_1125.jpg)\n",
      "(4032, 3024)\n",
      " - FUNCTION CALL: get_img_mimetype(IMG_1125.jpg)\n",
      "image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import mimetypes\n",
    "\n",
    "def get_img_mimetype () -> str:\n",
    "  \"\"\"Retrieve the mimetype of an image file.\"\"\"\n",
    "  image_file = img\n",
    "  print(f' - FUNCTION CALL: get_img_mimetype({image_file.filename})')\n",
    "\n",
    "  # The following line assumes that the image path is the current directory.\n",
    "  img_mime_type, img_encoding = mimetypes.guess_type(image_file.filename)\n",
    "  if img_mime_type == \"None\":\n",
    "    img_mime_type = \"image/\" + image_file.format\n",
    "\n",
    "  return img_mime_type\n",
    "\n",
    "def get_img_size () -> str:\n",
    "  \"\"\"Retrieve the height and width of an image file in pixels.\"\"\"\n",
    "  image_file = img\n",
    "  print(f' - FUNCTION CALL: get_img_size({image_file.filename})')\n",
    "\n",
    "  return image_file.size\n",
    "\n",
    "print(get_img_size())\n",
    "print(get_img_mimetype())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381d17d",
   "metadata": {},
   "source": [
    "Change the prompt and API call to use an interactive chat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b443f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tools = [get_img_size, get_img_mimetype]\n",
    "\n",
    "base_instruction = \"\"\"You are an image analyzing chatbot that can interact with image file tools. You will accept user requests \n",
    "containing images, use image tools to identify necessary file attributes, and return output from tool functions and then a JSON response containing file attributes and \n",
    "a catalog of the elements.\n",
    "\n",
    "Use the tool get_img_size to provide a list containing the W (width) and H (height) of the image in order, and use the tool get_img_mimetype \n",
    "to provide the image mimetype.\"\"\"\n",
    "\n",
    "instruction = base_instruction + few_shot_examples\n",
    "\n",
    "# Start a chat with automatic function calling enabled.\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=instruction,\n",
    "        tools=file_tools\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e7bdbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - FUNCTION CALL: get_img_size(IMG_1125.jpg)\n",
      " - FUNCTION CALL: get_img_mimetype(IMG_1125.jpg)\n",
      "\n",
      "```json\n",
      "{\n",
      "\"size\": \"4032Wx3024H\",\n",
      "\"mime-type\": \"image/jpeg\",\n",
      "\"contents\": \n",
      "[\n",
      "{   \n",
      "\"object\": \"turbine vent\", \n",
      "\"type\": \"roof turbine vent\", \n",
      "\"colors\": \"silver\",\n",
      "\"description\": \"a silver metal roof turbine vent\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"plumbing vent\", \n",
      "\"type\": \"roof plumbing vent\", \n",
      "\"colors\": \"grey\",\n",
      "\"description\": \"a short grey roof plumbing vent\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"shingles\", \n",
      "\"type\": \"roof shingles\", \n",
      "\"colors\": \"brown\",\n",
      "\"description\": \"brown asphalt roof shingles\" \n",
      "},\n",
      "{\n",
      "\"object\": \"tree\", \n",
      "\"type\": \"large tree\", \n",
      "\"colors\": \"green, brown\",\n",
      "\"description\": \"a very large tree with green leaves and brown branches\" \n",
      "},\n",
      "{\n",
      "\"object\": \"pole\", \n",
      "\"type\": \"utility pole\", \n",
      "\"colors\": \"grey\",\n",
      "\"description\": \"a grey wooden utility pole with power lines attached\" \n",
      "}\n",
      "]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "resp = chat.send_message(img)\n",
    "print(f\"\\n{resp.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bf6f9",
   "metadata": {},
   "source": [
    "Ask more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b1fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am unable to identify the manufacturer of any of the objects in the image using the available tools. I considered the turbine vent, plumbing vent, and shingles, but I am unable to determine their manufacturers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resp = chat.send_message(\"Can you identify the manufacturer of anything on the roof? What items are you considering?\")\n",
    "print(f\"\\n{resp.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7cc695",
   "metadata": {},
   "source": [
    "Send other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9f9ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - FUNCTION CALL: get_img_size(IMG_1106.jpg)\n",
      " - FUNCTION CALL: get_img_mimetype(IMG_1106.jpg)\n",
      "\n",
      "```json\n",
      "{\n",
      "\"size\": \"4032Wx3024H\",\n",
      "\"mime-type\": \"image/jpeg\",\n",
      "\"contents\": \n",
      "[\n",
      "{   \n",
      "\"object\": \"stop sign\", \n",
      "\"type\": \"stop sign\", \n",
      "\"colors\": \"red, white\",\n",
      "\"description\": \"a red and white stop sign at a street corner\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"street sign\", \n",
      "\"type\": \"street sign\", \n",
      "\"colors\": \"green, white\",\n",
      "\"description\": \"a green and white street sign reading 'Trestle Glen'\" \n",
      "},\n",
      "{   \n",
      "\"object\": \"fire hydrant\", \n",
      "\"type\": \"fire hydrant\", \n",
      "\"colors\": \"silver\",\n",
      "\"description\": \"a silver fire hydrant on the corner\" \n",
      "},\n",
      "{\n",
      "\"object\": \"parking meter\", \n",
      "\"type\": \"parking meter\", \n",
      "\"colors\": \"grey\",\n",
      "\"description\": \"a grey parking meter on the sidewalk\" \n",
      "},\n",
      "{\n",
      "\"object\": \"cars\", \n",
      "\"type\": \"cars\", \n",
      "\"colors\": \"various\",\n",
      "\"description\": \"several cars parked and driving on the street\" \n",
      "},\n",
      "{\n",
      "\"object\": \"tree\", \n",
      "\"type\": \"trees\", \n",
      "\"colors\": \"green\",\n",
      "\"description\": \"several green trees\" \n",
      "},\n",
      "{\n",
      "\"object\": \"building\", \n",
      "\"type\": \"building\", \n",
      "\"colors\": \"white, grey\",\n",
      "\"description\": \"a building in the background\" \n",
      "}\n",
      "]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "img.close()\n",
    "alt_images = ['IMG_1069.jpg', 'IMG_1106.jpg']\n",
    "img = PIL.Image.open(alt_images[1])\n",
    "\n",
    "resp = chat.send_message(img)\n",
    "print(f\"\\n{resp.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59941c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Okay, here's a list of the cars I can identify in the image:\n",
      "\n",
      "*   A grey sedan in the lower left corner\n",
      "*   Several cars driving on the street in the background, including a dark sedan and a light-colored car\n",
      "*   A silver SUV parked on the right side of the street\n",
      "*   A black Tesla parked near the SUV.\n"
     ]
    }
   ],
   "source": [
    "resp = chat.send_message(\"List the cars in the image.\")\n",
    "print(f\"\\n{resp.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ac2ef",
   "metadata": {},
   "source": [
    "## Postscript\n",
    "This exercise was my dipping my first toe in the water of modern Generative AI. Both the strengh of the technology and the challenges seem to **click** with my historical knowledge about AI and optimization projects at Tepper. I'm eager to do more.\n",
    "\n",
    "### Next Steps\n",
    "My next project was a private one, where I used python to create tools to clean out an overstuffed old mailbox in MBOX format. \n",
    "\n",
    "I went through the gyrations of trying small local models using **Ollama**:\n",
    "- qwen2.5-coder:latest\n",
    "- llama3.1:8b-instruct-q8_0\n",
    "- llama3.1:70b\n",
    "\n",
    "I wrote the tools code as I would have for *Gemini*, and rewrote for *Qwen* and rewrote for *Llama3.1* before recognizing that the smaller models didn't provide enough context to execute a combination of the natural language interaction and data processing.\n",
    "\n",
    "Waiting for llama3.1:70b I resolved to rewrite for Gemini, where it worked without a hitch. I realize now it's a better vibe coding exercise than something a small local model can assist me with. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a257e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
